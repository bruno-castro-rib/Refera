{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5456d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,plot_roc_curve,plot_precision_recall_curve\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17beaa",
   "metadata": {},
   "source": [
    "### Data Collection and Data Cleaning\n",
    "Foi utilizada a base de dados do ano 2020 conforme solicitado pela empresa. Algumas características foram identificadas dentro da base de dados obtida: \n",
    "\n",
    "    (1) São muitos parâmetros presentes, sendo necessário lidar com a tarefa de escolher os principais.\n",
    "\n",
    "    (2) Os dados são anonimizados, o que faz com que a análise fique principalmente baseada em scores de correlação, retirando a oportunidade de interpretação das features\n",
    "\n",
    "    (3) A base é constituida de Features Contínuas, sem variáveis categóricas ou ordinais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86eb1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções de tratamento de dados\n",
    "\n",
    "def file_correction(file):\n",
    "    file.replace('na',np.nan,inplace=True)\n",
    "    file[file.columns[1:]] = file[file.columns[1:]].astype('float64')\n",
    "    return file\n",
    "\n",
    "def file_encoding(file,categorical_columns=[]):\n",
    "    # Target encoding\n",
    "    y = LabelEncoder().fit_transform(file['class'])\n",
    "    file_features = file[file.columns[1:]]\n",
    "    # Features enconding\n",
    "    if len(categorical_columns) > 0:\n",
    "        Xcat = OneHotEncoder(drop='first').fit_transform(file_features[categorical_columns])\n",
    "        # Features normalization\n",
    "        non_categorical_columns = list(set(file_features.columns) - set(categorical_columns))\n",
    "        Xcont = file_features[non_categorical_columns].values\n",
    "        Xcont = StandardScaler().fit_transform(Xcont)\n",
    "        X = np.append(Xcont,Xcat,axis=1)\n",
    "    else:\n",
    "        X = file_features.values\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "    return y,X\n",
    "\n",
    "def nulls_proportion(data):\n",
    "    data = data.isna()\n",
    "    not_null = data.value_counts(normalize=True)[False]\n",
    "    return 1-not_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd366e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./Desafio DS Dados/data_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b074f60",
   "metadata": {},
   "source": [
    "### Tratamento de nulos\n",
    "\n",
    "Identificou-se em muitas colunas dentro da base de dados. Para fazer uma limpeza na base de dados, desconsiderou-se todos aqueles em que há uma proporção > 5% de nulos em sua constituição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_correction(df)\n",
    "less_5percent = df.apply(lambda x:nulls_proportion(x)) < 0.05\n",
    "columns_toConsider = less_5percent[less_5percent==True].index\n",
    "df = df[columns_toConsider]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5515c5",
   "metadata": {},
   "source": [
    "Como a proporção de nulos é baixa em relação à base de dados e as features têm caráter de variáveis contínuas, resolveu-se preencher estes dados com a média dos valores de cada coluna, com o objetivo de ter uma base de dados mais consistente e que não será afetada pelos nulos presentes nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56446e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[1:]] = df[df.columns[1:]].apply(lambda x:x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dfb6d",
   "metadata": {},
   "source": [
    "### Proporção das classes\n",
    "\n",
    "Identificou se que se trata de um problema de classificação em que a variável target é desbalanceada contendo somente 2% da classe minoritária (pos) e o restante da classe majoritária (neg). Por meio desta constatação, observa-se que na maioria dos casos os veículos são mandados para serem consertados sem estarem com nenhum tipo de problema de ar condicionado. \n",
    "\n",
    "Assim, o desafio de identificar um problema no ar torna-se maior pois há uma proporção menor de dados com informações sobre a classe mais importante para o problema.\n",
    "\n",
    "Para lidar com isto, sera usada uma técnica de resampling com o objetivo de corrigir a proporção dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a506999f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    0.977\n",
       "pos    0.023\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0777ccd",
   "metadata": {},
   "source": [
    "### Análise de correlação entre as features e target \n",
    "\n",
    "Observa-se que a variável target é uma variável categórica, e por este motivo técnicas de análise de correlação/associação devem ser aplicadas considerando o seguinte cenário:\n",
    "\n",
    "Target categórico x Feature contínua = ANOVA\n",
    "\n",
    "Este método foi aplicando duas funções da biblioteca sklean chamadas f_classif , que utiliza a técnica ANOVA e o SelectKBest que faz o trabalho de selecionar os parâmetros de acordo com o score selecionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "435c2dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vctor\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [74] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\vctor\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif,SelectKBest\n",
    "\n",
    "y,X = file_encoding(df)\n",
    "X = SelectKBest(f_classif,k=5).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d2c59",
   "metadata": {},
   "source": [
    "#### Criação do modelo\n",
    "\n",
    "Como se trata de uma base de dados desbalanceada, é necessário aplicar uma técnica de resampling. Considerando que a quantidade de dados da classe minoritária é muito pequena, escolheu-se o método de Oversample, que considera o tamanho da classe minoritária e replica a classe minoritária para ter o mesmo tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae44dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6164ba",
   "metadata": {},
   "source": [
    "#### Técnicas utilizadas\n",
    "\n",
    "Escolheu-se técnicas com diferentes níveis de complexidade e com características distintas entre si. Assim, pode-se ter maiores critérios para a escolha do modelo, não baseando-se somente no score final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "svm = SVC(probability=True)\n",
    "dtc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "logit.fit(X_train,y_train)\n",
    "svm.fit(X_train,y_train)\n",
    "dtc.fit(X_train,y_train)\n",
    "gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e865a",
   "metadata": {},
   "source": [
    "### Avaliaçao do modelo\n",
    "\n",
    "Utilizou-se para avaliar o modelo as métricas Precision e Recall bem como a curva ROC.\n",
    "\n",
    "#### ROC Curve\n",
    "Não foi possível fazer uma grande distinção entre os modelos pois todos tiveram bom desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76085fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [logit, svm, dtc,gb]\n",
    "ax = plt.gca()\n",
    "for i in classifiers:\n",
    "    plot_roc_curve(i, X, y, ax=ax)\n",
    "\n",
    "plt.title(\"ROC curve comparison\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838f072",
   "metadata": {},
   "source": [
    "#### Precision e Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021719a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), \n",
    "          RandomForestClassifier(max_depth=2, random_state=0),\n",
    "          SVC(),\n",
    "          GradientBoostingClassifier()]\n",
    "\n",
    "result = list()\n",
    "\n",
    "for model in models:\n",
    "    steps = [('over', RandomOverSampler()), ('model',model )]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1)\n",
    "    result.append(scores)\n",
    "    \n",
    "df_scores = pd.DataFrame(np.array(result).T,\n",
    "                         columns=['Logistic Regression','Random Forest','Suport Vector Machine','Gradient Boosting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.mean().round(4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e85e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_plot = pd.melt(df_scores)\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.stripplot(x='variable',y='value',data=df_scores_plot)\n",
    "plt.title('Precision distribution for crossvalidation')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), \n",
    "          RandomForestClassifier(max_depth=2, random_state=0),\n",
    "          SVC(),\n",
    "          GradientBoostingClassifier()]\n",
    "\n",
    "result = list()\n",
    "\n",
    "for model in models:\n",
    "    steps = [('over', RandomOverSampler()), ('model',model )]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    result.append(scores)\n",
    "    \n",
    "df_scores = pd.DataFrame(np.array(result).T,\n",
    "                         columns=['Logistic Regression','Random Forest','Suport Vector Machine','Gradient Boosting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a958fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.mean().round(4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_plot = pd.melt(df_scores)\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.stripplot(x='variable',y='value',data=df_scores_plot)\n",
    "plt.title('Recall distribution for crossvalidation')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs,axs=plt.subplots(2,2,figsize=[10,8])\n",
    "count = 0\n",
    "for i,model in ('Logistic Regression',logit),('Support Vector Machine',svm),('Random Forest',dtc),('Gradient Boosting',gb):\n",
    "    conf_matrix = confusion_matrix(y_test,model.predict(X_test))\n",
    "    sns.heatmap(conf_matrix,annot=True,ax=axs.flat[count])\n",
    "    axs.flat[count].set_title(i)\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
